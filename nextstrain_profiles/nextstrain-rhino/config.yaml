configfile:
  - defaults/parameters.yaml
  - nextstrain_profiles/nextstrain/builds.yaml

cores: 20
keep-going: True
printshellcmds: True
show-failed-logs: True
reason: True
stats: stats.json

# Custom settings for running builds on a SLURM cluster with DRMAA bindings.

# Use conda environments for augur and related binaries. This is important in a
# cluster environment where Docker is not available and Singularity may be
# prohibitively complicated to setup.
use-conda: True

# Cluster-specific settings for resources required by any rule. This file
# provides default resources for all rules and allows users to specify resources
# per rule by name. An important resource for the Hutch cluster is the requested
# "partition". Jobs submitted to the "restart" partition will start running
# almost immediately, but they may also be killed at any moment when someone
# else needs those resources. This is analogous to the spot resources on AWS.
cluster-config: nextstrain_profiles/nextstrain-rhino/cluster.json

# Submit jobs to the cluster with SLURM's DRMAA bindings. The string associated
# with this key tells SLURM how to map cluster resources with those defined in
# the cluster config above.
default-resources: mem_mb=256
drmaa: " {cluster.partition} --nodes=1 --ntasks=1 --mem={resources.mem_mb} --cpus-per-task={threads} --time={cluster.time}"

# NOTE: Use the settings from the next block if DRMAA is not an option.
# Use a custom script to evaluate the status of jobs on the cluster. This
# prevents jobs from dying silently and not being caught by Snakemake.
#cluster: "sbatch {cluster.partition} --nodes=1 --ntasks=1 --mem={cluster.memory} --cpus-per-task={cluster.cores} --tmp={cluster.disk} --time={cluster.time}"
#cluster-status: "slurm-status.py"

# Wait a fixed number of seconds for missing files since the cluster file system
# can be quite slow and the workflow can fail unnecessarily due to this latency.
latency-wait: 60

# Restart failed jobs multiple times since the cluster may kill our jobs
# whenever another job with higher priority comes along.
restart-times: 3

# Limit job submission and status checks
max-jobs-per-second: 1
max-status-checks-per-second: 1

# Limit cores used on the head node where Snakemake is running.
local-cores: 1

# Set the name for the job as display in the cluster queue.
jobname: "{rulename}.{jobid}.sh"

# Set the number of threads (cores, actually) that the tree rule or other rules
# should use on the cluster. These settings override the hardcoded values in the
# Snakemake rules. For the rhino cluster, it is difficult to get a node with
# more than 4 cores.
set-threads:
  - align=8
  - tree=8
